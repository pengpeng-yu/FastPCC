from typing import List, Tuple, Dict, Union, Callable
import math

import torch
import torch.nn as nn
import torch.distributions
from torch.distributions import Distribution

from .utils import quantization_offset
from .rans_coder import IndexedRansCoder


class DistributionQuantizedCDFTable(nn.Module):
    """
    Provide function that can generate flat quantized CDF table
    used by range coder.

    If "cdf" is available, use "cdf" to generate.
    Otherwise, base distribution should have derivable
    "icdf" or cdf-related functions for tails estimation.
    Then CDF table is generated by tails and "prob" function.

    The names of parameters for tails estimation end with "aux_param",
    which are supposed to be specially treated in training.
    """
    def __init__(self,
                 base: Distribution,
                 init_scale: float,
                 tail_mass: float,
                 lower_bound: Union[int, torch.Tensor],
                 upper_bound: Union[int, torch.Tensor],
                 coding_batch_size: int,
                 overflow_coding: bool,
                 bottleneck_scaler: int
                 ):
        super(DistributionQuantizedCDFTable, self).__init__()
        self.base = base
        self.init_scale = init_scale
        self.tail_mass = tail_mass
        self.coding_batch_size = coding_batch_size
        self.overflow_coding = overflow_coding
        self.bottleneck_scaler = bottleneck_scaler

        is_valid = upper_bound >= lower_bound
        if isinstance(is_valid, torch.Tensor):
            is_valid = torch.all(is_valid).item()
        self.if_estimate_tail = not is_valid

        if self.if_estimate_tail:
            self.estimate_tail()
        else:
            self.register_buffer(
                'lower_bound',
                lower_bound if isinstance(lower_bound, torch.Tensor)
                else torch.tensor(lower_bound, dtype=torch.int32).expand(self.batch_shape),
                persistent=False
            )
            self.register_buffer(
                'upper_bound',
                upper_bound if isinstance(upper_bound, torch.Tensor)
                else torch.tensor(upper_bound, dtype=torch.int32).expand(self.batch_shape),
                persistent=False
            )

        self.cdf_list: List[List[int]] = [[]]
        self.cdf_offset_list: List[int] = []
        self.requires_updating_cdf_table: bool = True
        self.range_coder = IndexedRansCoder(self.overflow_coding, self.coding_batch_size)

        if len(base.event_shape) != 0:
            raise NotImplementedError

    def update_base(self, new_base: Distribution):
        assert type(new_base) is type(self.base)
        assert new_base.batch_shape == self.base.batch_shape
        assert new_base.event_shape == self.base.event_shape
        self.base = new_base

    def estimate_tail(self) -> None:
        self.e_input_values = []  # type: List[torch.Tensor]
        self.e_functions = []  # type: List[Callable[[torch.Tensor], torch.Tensor]]
        self.e_target_values = []  # type: List[Union[int, float, torch.Tensor]]

        try:
            _ = self.base.icdf(0.5)
        except NotImplementedError:
            icdf_available = False
        else:
            icdf_available = True

        # Lower tail estimation.
        if icdf_available:
            self.lower_tail_fn = lambda: self.base.icdf(self.tail_mass / 2)
            self.lower_tail_aux_param = None

        elif hasattr(self.base, 'lower_tail'):
            self.lower_tail_fn = lambda: self.base.lower_tail(self.tail_mass)
            self.lower_tail_aux_param = None

        else:
            self.lower_tail_fn = None
            self.lower_tail_aux_param = nn.Parameter(
                torch.full(self.batch_shape,
                           fill_value=-self.init_scale,
                           dtype=torch.float))
            self.e_input_values.append(self.lower_tail_aux_param)
            if hasattr(self.base, 'logits_cdf_for_estimation'):
                self.e_functions.append(
                    lambda *args, **kwargs: self.base.logits_cdf_for_estimation(*args, **kwargs)
                )
                self.e_target_values.append(
                    math.log(self.tail_mass / 2 / (1 - self.tail_mass / 2))
                )
            elif hasattr(self.base, 'log_cdf_for_estimation'):
                self.e_functions.append(
                    lambda *args, **kwargs: self.base.log_cdf_for_estimation(*args, **kwargs)
                )
                self.e_target_values.append(math.log(self.tail_mass / 2))
            else: raise NotImplementedError

        # Upper tail estimation.
        if icdf_available:
            self.upper_tail_fn = lambda: self.base.icdf(1 - self.tail_mass / 2)
            self.upper_tail_aux_param = None

        elif hasattr(self.base, 'upper_tail'):
            self.upper_tail_fn = lambda: self.base.upper_tail(self.tail_mass)
            self.upper_tail_aux_param = None

        else:
            self.upper_tail_fn = None
            self.upper_tail_aux_param = nn.Parameter(
                torch.full(self.batch_shape,
                           fill_value=self.init_scale,
                           dtype=torch.float))
            self.e_input_values.append(self.upper_tail_aux_param)
            if hasattr(self.base, 'logits_cdf_for_estimation'):
                self.e_functions.append(
                    lambda *args, **kwargs: self.base.logits_cdf_for_estimation(*args, **kwargs)
                )
                self.e_target_values.append(
                    -math.log(self.tail_mass / 2 / (1 - self.tail_mass / 2))
                )
            elif hasattr(self.base, 'log_survival_function_for_estimation'):
                self.e_functions.append(
                    lambda *args, **kwargs: self.base.log_survival_function_for_estimation(*args, **kwargs)
                )
                self.e_target_values.append(math.log(self.tail_mass / 2))
            else: raise NotImplementedError

    def lower_tail(self):
        if self.lower_tail_aux_param is not None:
            return self.lower_tail_aux_param
        else:
            return self.lower_tail_fn()

    def upper_tail(self):
        if self.upper_tail_aux_param is not None:
            return self.upper_tail_aux_param
        else:
            return self.upper_tail_fn()

    def mean(self):
        return self.base.mean()

    @property
    def batch_shape(self):
        return self.base.batch_shape

    @property
    def event_shape(self):
        return torch.Size([])

    @property
    def batch_numel(self):
        return self.base.batch_shape.numel()

    @property
    def batch_ndim(self):
        return len(self.base.batch_shape)

    def prob(self, value):
        if hasattr(self.base, 'prob'):
            return self.base.prob(value)
        else:
            raise NotImplementedError

    def log_prob(self, value):
        return self.base.log_prob(value)

    def aux_loss(self):
        """
        aux_loss is supposed to be minimized during training to
        estimate distribution tails, which is necessary for CDF table
        generation using "prob" function.
        Distribution with learnable params is supposed to have
        "stop_gradient" arg in their "e_functions".
        """
        if not self.if_estimate_tail:
            return None
        else:
            loss = []
            for i, f, t in zip(self.e_input_values, self.e_functions, self.e_target_values):
                try:
                    # Stop gradient of learnable params in distribution
                    # by trying to send a flag.
                    # This try-except block will cause unexpected behavior
                    # if a distribution with learnable params but without
                    # stop_gradient arg is used.
                    p = f(i, stop_gradient=True)
                except TypeError:
                    p = f(i)
                loss.append(torch.abs(p - t).mean())
            return sum(loss)

    @torch.no_grad()
    def build_quantized_cdf_table(self):
        offset = quantization_offset(self.base)

        if not self.if_estimate_tail:
            minima = self.lower_bound * self.bottleneck_scaler
            maxima = self.upper_bound * self.bottleneck_scaler

        else:
            lower_tail = self.lower_tail() * self.bottleneck_scaler
            upper_tail = self.upper_tail() * self.bottleneck_scaler

            # minima < lower_tail - offset
            # maxima > upper_tail - offset
            # Sightly increase range of pmfs to generate better cdf tables.
            minima = torch.floor(lower_tail - offset).to(torch.int32) - 10
            maxima = torch.ceil(upper_tail - offset).to(torch.int32) + 10
            # For stability.
            maxima.clip_(minima)

        # PMF starting positions and lengths.
        pmf_start = minima + offset
        pmf_length = maxima - minima + 1

        # Sample the densities in the computed ranges, possibly computing more
        # samples than necessary at the upper end.
        max_length = pmf_length.max().item()
        if max_length > 2048:
            print(f"Very wide PMF with {max_length} elements may lead to out of memory issues. "
                  "Consider priors with smaller dispersion or increasing `tail_mass` parameter.")
        samples = torch.arange(max_length, device=pmf_start.device)
        samples = samples.reshape(max_length, *[1] * len(self.base.batch_shape))
        samples = samples + pmf_start[None, ...]  # broadcast

        try:
            pmf = self.prob(samples / self.bottleneck_scaler)
        except NotImplementedError:
            pmf = torch.exp(self.log_prob(samples / self.bottleneck_scaler))

        # Collapse batch dimensions of distribution.
        pmf = pmf.reshape(max_length, -1).T
        pmf_length = pmf_length.reshape(-1)
        cdf_offset = minima.reshape(-1)

        pmf_list = []
        for i in range(len(pmf_length)):
            single_pmf = pmf[i][: pmf_length[i]].tolist()
            pmf_list.append(single_pmf)
        self.cdf_list, self.cdf_offset_list = self.range_coder.init_with_pmfs(
            pmf_list, self.cdf_offset_list)
        self.requires_updating_cdf_table = False

    def get_extra_state(self):
        return self.cdf_list, self.cdf_offset_list, self.requires_updating_cdf_table

    def set_extra_state(self, state):
        if state[2]:
            print('Warning: cached cdf table in state dict requires updating.\n'
                  'You need to call model.eval() to build it after loading state dict '
                  'before any inference.')
        else:
            self.cdf_list, self.cdf_offset_list, self.requires_updating_cdf_table = state
            self.range_coder.init_with_quantized_cdfs(self.cdf_list, self.cdf_offset_list)

    def train(self, mode: bool = True):
        """
        Use model.train() to invalidate cached cdf table.
        Use model.eval() to call build_quantized_cdf_table().
        """
        if mode is True:
            self.requires_updating_cdf_table = True
        else:
            if self.requires_updating_cdf_table:
                self.build_quantized_cdf_table()
        return super(DistributionQuantizedCDFTable, self).train(mode=mode)


class ContinuousEntropyModelBase(nn.Module):
    def __init__(self,
                 prior: Distribution,
                 coding_ndim: int,
                 bottleneck_process: str,
                 bottleneck_scaler: int,
                 init_scale: float,
                 tail_mass: float,
                 lower_bound: Union[int, torch.Tensor],
                 upper_bound: Union[int, torch.Tensor],
                 batch_shape: torch.Size,
                 overflow_coding: bool):
        super(ContinuousEntropyModelBase, self).__init__()
        # "self.prior" is supposed to be able to generate
        # flat quantized CDF table used by range coder.
        self.prior: DistributionQuantizedCDFTable = DistributionQuantizedCDFTable(
            base=prior,
            init_scale=init_scale,
            tail_mass=tail_mass,
            lower_bound=lower_bound,
            upper_bound=upper_bound,
            coding_batch_size=batch_shape.numel(),
            overflow_coding=overflow_coding,
            bottleneck_scaler=bottleneck_scaler
        )
        self.quantize_bottleneck = 'quantization' in bottleneck_process
        if self.quantize_bottleneck:
            bottleneck_process = bottleneck_process.replace('quantization', '', 1)
        self.perturb_bottleneck = 'noise' in bottleneck_process
        if self.perturb_bottleneck:
            bottleneck_process = bottleneck_process.replace('noise', '', 1)
        assert bottleneck_process in (',', '_', ' ', '+', ''), \
            f'Unexpected bottleneck_process: {bottleneck_process}'
        self.coding_ndim = coding_ndim

    def perturb(self, x: torch.Tensor) -> torch.Tensor:
        if not hasattr(self, "_noise"):
            setattr(self, "_noise", torch.empty(x.shape, dtype=torch.float, device=x.device))
        self._noise.resize_(x.shape)
        self._noise.uniform_(-0.5, 0.5)
        x = x + self._noise
        return x

    def process(self, x: torch.Tensor) -> torch.Tensor:
        if self.quantize_bottleneck is True:
            x = x + (x.detach().round() - x.detach())
        if self.perturb_bottleneck is True:
            x = self.perturb(x)
        return x

    @torch.no_grad()
    def quantize(self, x: torch.Tensor, offset=None) -> Tuple[torch.Tensor, torch.Tensor]:
        if offset is None: offset = quantization_offset(self.prior.base)
        x -= offset
        torch.round_(x)
        quantized_x = x.to(torch.int32)
        x += offset
        return quantized_x, x

    @torch.no_grad()
    def dequantize(self, x: torch.Tensor, offset=None) -> torch.Tensor:
        if offset is None: offset = quantization_offset(self.prior.base)
        if isinstance(offset, torch.Tensor) and x.device != offset.device:
            x = x.to(offset.device)
        x += offset
        return x.to(torch.float)

    def forward(self, *args, **kwargs) \
            -> Union[Tuple[torch.Tensor, Dict[str, torch.Tensor]],
                     Tuple[torch.Tensor, Dict[str, torch.Tensor], List]]:
        raise NotImplementedError

    def compress(self, *args, **kwargs):
        raise NotImplementedError

    def decompress(self, *args, **kwargs):
        raise NotImplementedError
