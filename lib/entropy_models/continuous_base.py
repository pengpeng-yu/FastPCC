from typing import List, Tuple, Dict, Union, Callable
import math
from functools import partial

import torch
import torch.nn as nn
import torch.distributions
from torch.distributions import Distribution
from compressai._CXX import pmf_to_quantized_cdf as _pmf_to_quantized_cdf
from compressai import ans

from .utils import quantization_offset


def batched_pmf_to_quantized_cdf(pmf: torch.Tensor,
                                 overflow: torch.Tensor,
                                 pmf_length: torch.Tensor,
                                 max_length: int,
                                 entropy_coder_precision: int = 16):
    """
    :param pmf: (channels, max_length) float
    :param overflow: (channels, ) float
    :param pmf_length: (channels, ) int32
    :param max_length: max length of pmf, int
    :param entropy_coder_precision
    :return: quantized cdf (channels, max_length + 2)
    """
    cdf = torch.zeros((len(pmf_length), max_length + 2),
                      dtype=torch.int32, device=pmf.device)
    for i, p in enumerate(pmf):
        prob = torch.cat((p[: pmf_length[i]], overflow[i, None]), dim=0)
        _cdf = _pmf_to_quantized_cdf(prob.tolist(), entropy_coder_precision)
        cdf[i, : len(_cdf)] = torch.tensor(_cdf, dtype=torch.int32)
    return cdf


class DistributionQuantizedCDFTable(nn.Module):
    """
    Provide function that can generate flat quantized CDF table
    used by range coder.

    If "cdf" is available, use "cdf" to generate.
    Otherwise, base distribution should have derivable
    "icdf" or cdf-related function for tails estimation.
    Then CDF table is generated by tails and "prob" function.

    The names of parameters for tails estimation end with "aux_param",
    which are supposed to be specially treated in training.
    """
    def __init__(self,
                 base: Distribution,
                 tail_mass: float = 2 ** -8,
                 init_scale: int = 10,
                 cdf_precision: int = 16,
                 ):
        super(DistributionQuantizedCDFTable, self).__init__()
        self.base = base
        self.tail_mass = tail_mass
        self.init_scale = init_scale
        self.cdf_precision = cdf_precision
        self.cached_cdf_table = None  # TODO
        self.cached_cdf_length = None
        self.cached_cdf_offset = None

        if len(base.event_shape) != 0:
            raise NotImplementedError

        try:
            _ = self.base.cdf(0.5)
        except NotImplementedError:
            self.base_cdf_available = False
        else:
            self.base_cdf_available = True

        if not self.base_cdf_available:
            self.estimate_tail()

    def estimate_tail(self) -> None:
        self.e_input_values = []  # type: List[torch.Tensor]
        self.e_functions = []  # type: List[Callable[[torch.Tensor], torch.Tensor]]
        self.e_target_values = []  # type: List[Union[int, float, torch.Tensor]]

        try:
            _ = self.base.icdf(0.5)
        except NotImplementedError:
            icdf_available = False
        else:
            icdf_available = True

        # lower tail estimation
        if icdf_available:
            self.lower_tail_fn = partial(self.base.icdf, self.tail_mass / 2)
            self.lower_tail_aux_param = None
        elif hasattr(self.base, 'lower_tail'):
            self.lower_tail_fn = partial(self.base.lower_tail, self.tail_mass)
            self.lower_tail_aux_param = None
        else:
            self.lower_tail_fn = None
            self.lower_tail_aux_param = nn.Parameter(
                torch.full(self.batch_shape,
                           fill_value=-self.init_scale,
                           dtype=torch.float))
            self.e_input_values.append(self.lower_tail_aux_param)
            try:
                self.e_functions.append(self.base.logits_cdf_for_estimation)
                self.e_target_values.append(
                    math.log(self.tail_mass / 2 / (1 - self.tail_mass / 2))
                )
            except AttributeError:
                self.e_functions.append(self.base.log_cdf_for_estimation)
                self.e_target_values.append(math.log(self.tail_mass / 2))

        # upper tail estimation
        if icdf_available:
            self.upper_tail_fn = partial(self.base.icdf, 1 - self.tail_mass / 2)
            self.upper_tail_aux_param = None
        elif hasattr(self.base, 'upper_tail'):
            self.upper_tail_fn = partial(self.base.upper_tail, self.tail_mass)
            self.upper_tail_aux_param = None
        else:
            self.upper_tail_fn = None
            self.upper_tail_aux_param = nn.Parameter(
                torch.full(self.batch_shape,
                           fill_value=self.init_scale,
                           dtype=torch.float))
            self.e_input_values.append(self.upper_tail_aux_param)
            try:
                self.e_functions.append(self.base.logits_cdf_for_estimation)
                self.e_target_values.append(
                    -math.log(self.tail_mass / 2 / (1 - self.tail_mass / 2))
                )
            except AttributeError:
                self.e_functions.append(self.base.log_survival_function_for_estimation)
                self.e_target_values.append(math.log(self.tail_mass / 2))

    def lower_tail(self):
        if self.lower_tail_aux_param is not None:
            return self.lower_tail_aux_param
        else:
            return self.lower_tail_fn()

    def upper_tail(self):
        if self.upper_tail_aux_param is not None:
            return self.upper_tail_aux_param
        else:
            return self.upper_tail_fn()

    def mean(self):
        return self.base.mean()

    @property
    def batch_shape(self):
        return self.base.batch_shape

    @property
    def event_shape(self):
        return torch.Size([])

    @property
    def batch_numel(self):
        return self.base.batch_shape.numel()

    @property
    def batch_ndim(self):
        return len(self.base.batch_shape)

    def log_prob(self, value):
        return self.base.log_prob(value)

    def aux_loss(self):
        """
        aux_loss is supposed to be minimized during training to
        estimate distribution tails, which is necessary for CDF table
        generation using "prob" function.
        """
        if self.e_input_values == self.e_functions == self.e_target_values == []:
            return 0
        else:
            loss = []
            for i, f, t in zip(self.e_input_values, self.e_functions, self.e_target_values):
                try:
                    # Stop gradient of learnable params in distribution
                    # by trying to send a flag.
                    p = f(i, stop_gradient=True)
                except TypeError:
                    p = f(i)
                loss.append(torch.abs(p - t).mean())
            return sum(loss)

    @torch.no_grad()
    def build_quantized_cdf_table(self, force=False):
        if not force and self.cached_cdf_table is not None:
            return

        if self.base_cdf_available:
            raise NotImplementedError  # TODO

        else:
            # TODO(jonycgn, relational): Consider not using offset when soft quantization is used.
            offset = quantization_offset(self.base)

            lower_tail = self.lower_tail()
            upper_tail = self.upper_tail()

            # minima < lower_tail - offset
            # maxima > upper_tail - offset
            minima = torch.floor(lower_tail - offset).to(torch.int32)
            maxima = torch.ceil(upper_tail - offset).to(torch.int32)

            # PMF starting positions and lengths.
            pmf_start = minima + offset
            pmf_length = maxima - minima + 1

            # Sample the densities in the computed ranges, possibly computing more
            # samples than necessary at the upper end.
            max_length = pmf_length.max().item()
            if max_length > 2048:
                print("Very wide PMF with %d elements may lead to out of memory issues. "
                      "Consider priors with smaller dispersion or increasing `tail_mass` "
                      "parameter.", int(max_length))
            samples = torch.arange(max_length, device=pmf_start.device)
            samples = samples.reshape(max_length,
                                      *[1] * len(self.base.batch_shape))
            samples = samples + pmf_start[None, ...]  # broadcast

            pmf = self.base.prob(samples)

            # Collapse batch dimensions of distribution.
            pmf = pmf.reshape(max_length, -1).T
            overflow = (1 - pmf.sum(dim=1)).clamp(max=0)
            pmf_length = pmf_length.reshape(-1)

            cdf = batched_pmf_to_quantized_cdf(pmf, overflow,
                                               pmf_length, max_length,
                                               self.cdf_precision)
            cdf_length = pmf_length + 2
            cdf_offset = minima

            self.cached_cdf_table = cdf
            self.cached_cdf_length = cdf_length
            self.cached_cdf_offset = cdf_offset

    def train(self, mode: bool = True):
        """
        Use model.eval() to call build_quantized_cdf_table().
        """
        if mode is False:
            self.build_quantized_cdf_table(force=True)  # TODO: call build function independently
        return super(DistributionQuantizedCDFTable, self).train(mode=mode)


class ContinuousEntropyModelBase(nn.Module):
    def __init__(self,
                 prior: Distribution,
                 coding_ndim: int,
                 tail_mass: float = 2 ** -8,
                 init_scale: int = 10,
                 range_coder_precision: int = 16):
        super(ContinuousEntropyModelBase, self).__init__()
        # "self.prior" is supposed to be able to generate
        # flat quantized CDF table used by range coder.
        self.prior = DistributionQuantizedCDFTable(
            base=prior,
            tail_mass=tail_mass,
            init_scale=init_scale,
            cdf_precision=range_coder_precision
        )
        self.coding_ndim = coding_ndim
        self.range_coder_precision = range_coder_precision
        self.range_encoder = ans.RansEncoder()
        self.range_decoder = ans.RansDecoder()

        assert self.coding_ndim >= self.prior.batch_ndim
        if self.range_coder_precision != 16:
            raise NotImplementedError

    def perturb(self, x: torch.Tensor) -> torch.Tensor:
        if not hasattr(self, "_noise"):
            setattr(self, "_noise", torch.empty(x.shape, dtype=torch.float, device=x.device))
        self._noise.resize_(x.shape)
        self._noise.uniform_(-0.5, 0.5)
        return x + self._noise

    @torch.no_grad()
    def quantize(self, x: torch.Tensor, offset=None) -> torch.Tensor:
        if offset is None: offset = quantization_offset(self.prior)
        x -= offset
        return torch.round(x).to(torch.int32)

    @torch.no_grad()
    def dequantize(self, x: torch.Tensor, offset=None) -> torch.Tensor:
        if offset is None: offset = quantization_offset(self.prior)
        if isinstance(offset, torch.Tensor) and x.device != offset.device:
            x = x.to(offset.device)
        x += offset
        return x.to(torch.float)

    def forward(self, *args, **kwargs) \
            -> Union[Tuple[torch.Tensor, Dict[str, torch.Tensor]],
                     Tuple[torch.Tensor, Dict[str, torch.Tensor], List]]:
        raise NotImplementedError

    def compress(self, *args, **kwargs):
        raise NotImplementedError

    def decompress(self, *args, **kwargs):
        raise NotImplementedError
